{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Михаил Ховричев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки выбран текст Дагласа Хофштадтера \"Глаз разума\". Необходимо произвести извлечение устойчивых биграмм (коллокаций).\n",
    "\n",
    "Для морфологического анализа пользуюсь pymorphy2.\n",
    "\n",
    "NLTK для предобработки, быстрого выделения биграмм, быстрого подсчёта различных характеристик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from tabulate import tabulate\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Dice's measure \"\"\"\n",
    "def mydice(Fa1, Fa2, Fa12):\n",
    "    return 2*Fa12/(Fa1+Fa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Mutual Information \"\"\"\n",
    "def myMI(N, Fa1, Fa2, Fa12):\n",
    "    return math.log2((N*Fa12)/(Fa1*Fa2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open('mindeye.txt', 'r')\n",
    "tokens = []\n",
    "for line in text.readlines():\n",
    "    tokens+=nltk.word_tokenize(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь производим предобработку текста. Удаляем токены, являющиеся символами, знаками препинания, числами.\n",
    "Также избавляемся от служебных частей речи (биграммы, где одна из частей речи -- служебная, практически никогда не являются коллокациями), местоимений и местоимённых прилагательных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "badPOS = {'NPRO','PREP', 'CONJ', 'PRCL', 'INTJ','PNCT', 'NUMB', 'ROMN', 'UNKN', 'Apro'}\n",
    "sw = stopwords.words(\"russian\") # список стоп-слов из NLTK\n",
    "\n",
    "# Списковое включение получается слишком длинным, поэтому вынесем в отдельные лямбда-функции \n",
    "# transform(token) -- то, что заносится в список\n",
    "# condition(token) -- предикат на токен\n",
    "\n",
    "transform = lambda x: morph.parse(x)[0]\n",
    "condition = lambda x: not [item for item in badPOS if item in str(transform(x).tag)] and transform(x).word not in sw\n",
    "\n",
    "words = [transform(token) for token in tokens if condition(token)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем количество словоупотреблений после чистки и количество словоформ.\n",
    "\n",
    "Далее производится лемматизация с помощью приведения каждого слова к его нормальному виду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество словоупотреблений после чистки: 81449\n",
      "Количество словоформ: 25353\n",
      "['введение', 'видеть', 'луна', 'восходить', 'восток', 'луна', 'восходить', 'запад', 'смотреть', 'два', 'луна', 'плыть', 'навстречу', 'друг', 'друг', 'чёрный', 'холодное', 'небо', 'вскоре', 'встретиться'] "
     ]
    }
   ],
   "source": [
    "print(\"Количество словоупотреблений после чистки:\", len(words))\n",
    "print(\"Количество словоформ:\", len(set(words)))\n",
    "lemmatized = [word.normal_form for word in words]\n",
    "\n",
    "print(lemmatized[:20], end=' ') # выведем первые 20 лемм, просто посмотреть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lem_fd = nltk.FreqDist(lemmatized)\n",
    "ranks = {}\n",
    "\n",
    "for (i,w) in enumerate(lem_fd.most_common()):\n",
    "    ranks[w[0]] = i+1\n",
    "\n",
    "W = lem_fd.max() # Самое частотное слово (мочь)\n",
    "Fw = lem_fd[W] # Абсолютная частота"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, самой частотной леммой оказалось \"мочь\".\n",
    "\n",
    "С помощью NLTK создадим список биграмм. Затем посчитаем их абсолютные частоты. Выведем слова, чаще всего встречаемые вместе с \"мочь\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('сказать', 26),\n",
       " ('представить', 17),\n",
       " ('понять', 14),\n",
       " ('сделать', 13),\n",
       " ('увидеть', 12),\n",
       " ('вообразить', 12),\n",
       " ('спросить', 9),\n",
       " ('объяснить', 9),\n",
       " ('создать', 9),\n",
       " ('уверить', 9),\n",
       " ('подумать', 9),\n",
       " ('иметь', 9),\n",
       " ('узнать', 9),\n",
       " ('существовать', 9),\n",
       " ('машина', 8)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgs = nltk.bigrams(lemmatized)\n",
    "\n",
    "bgs_fd = nltk.ConditionalFreqDist(bgs)\n",
    "bgs_fd[W].most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь извлечём список самых устойчивых биграмм по мере Дайса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коллокация           Dice\n",
      "-----------------  ------\n",
      "мочь сказать       0.0438\n",
      "мочь представить   0.0342\n",
      "мочь понять        0.0284\n",
      "мочь вообразить    0.027\n",
      "мочь сделать       0.0258\n",
      "мочь увидеть       0.0252\n",
      "мочь уверить       0.021\n",
      "мочь спросить      0.0199\n",
      "мочь подумать      0.0199\n",
      "мочь объяснить     0.0197\n",
      "мочь создать       0.0196\n",
      "мочь узнать        0.0193\n",
      "мочь существовать  0.0166\n",
      "мочь иметь         0.0152\n",
      "мочь найти         0.0152\n",
      "мочь говорить      0.0145\n",
      "мочь считать       0.0143\n",
      "мочь ожидать       0.0138\n",
      "мочь обнаружить    0.0138\n",
      "мочь заставить     0.0137\n"
     ]
    }
   ],
   "source": [
    "dicelist = sorted([(k,round(mydice(Fw, lem_fd[k], v),4)) for k,v in bgs_fd[W].items()], key= lambda x: x[1], reverse=True)\n",
    "dicelist_top = dicelist[:20]\n",
    "\n",
    "dicetable = [[W+' '+lemW, dice_val] for (lemW, dice_val) in dicelist_top]\n",
    "\n",
    "print(tabulate(dicetable, headers=[\"Коллокация\", \"Dice\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое по мере Mutual Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Коллокация                MI\n",
      "--------------------  ------\n",
      "мочь вживаться        6.6028\n",
      "мочь утешительный     6.6028\n",
      "мочь пролить          6.6028\n",
      "мочь придумывать      6.6028\n",
      "мочь посыпаться       6.6028\n",
      "мочь цапнуть          6.6028\n",
      "мочь прокормить       6.6028\n",
      "мочь отрепетировать   6.6028\n",
      "мочь группироваться   6.6028\n",
      "мочь калибровать      6.6028\n",
      "мочь изобретательно   6.6028\n",
      "мочь пересказать      6.6028\n",
      "мочь пустить          6.6028\n",
      "мочь взаимопроникать  6.6028\n",
      "мочь наставить        6.6028\n",
      "мочь нестабильный     6.6028\n",
      "мочь вертеть          6.6028\n",
      "мочь уподобиться      6.6028\n",
      "мочь обездушить       6.6028\n",
      "мочь отпустить        6.6028\n"
     ]
    }
   ],
   "source": [
    "milist = sorted([(k,myMI(len(lemmatized), Fw, lem_fd[k], v)) for k,v in bgs_fd[W].items()], key= lambda x: x[1], reverse=True)\n",
    "milist_top = milist[:20]\n",
    "\n",
    "mitable = [[W+' '+lemW, mi_val] for (lemW, mi_val) in milist_top]\n",
    "\n",
    "print(tabulate(mitable, headers=[\"Коллокация\", \"MI\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат по мере Дайса получился весьма предсказуемым. Устойчивее те биграммы, в которых с нашим основным \"мочь\" второе слово встречается чаще. По мере \"общей информации\" (по крайней мере, по той формуле, которая представлена на слайде) результат получился странный и неочевидный. Большое количество результатов с одинаковым значением меры легко объясняется тем, что в тексте после чистки осталось не так много словоупотреблений ~80k, а словоформ и того меньше. Всё-таки даже книга не сравнится с большим корпусом. Однако всё было сделано правильно, поэтому странный результат для меры общей информации странен лишь по природе текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
